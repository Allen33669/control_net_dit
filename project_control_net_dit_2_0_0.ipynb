{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30DDyP3ams3e"
      },
      "source": [
        "Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLf1tsEqn8Jt"
      },
      "source": [
        "Licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. You may not use this file except in compliance with the License. You may obtain a copy of the License at http://creativecommons.org/licenses/by-nc/4.0/ Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8a5Y1rwXd9s"
      },
      "source": [
        "This project includes code from<br>\n",
        "(https://github.com/lllyasviel/controlnet), licensed under the Apache License Version 2.0.<br>\n",
        "(https://github.com/facebookresearch/DiT), licensed under the Attribution-NonCommercial 4.0 International.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6LiUi0hYY2U"
      },
      "source": [
        "paper:<br>\n",
        "Adding Conditional Control to Text-to-Image Diffusion Models https://arxiv.org/abs/2302.05543.<br>\n",
        "Scalable Diffusion Models with Transformers https://arxiv.org/abs/2212.09748."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optuna:<br>\n",
        "@inproceedings{akiba2019optuna,\n",
        "  title={{O}ptuna: A Next-Generation Hyperparameter Optimization Framework},\n",
        "  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},\n",
        "  booktitle={The 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n",
        "  pages={2623--2631},\n",
        "  year={2019}\n",
        "}<br>\n",
        "License:<br>\n",
        "MIT License (see LICENSE).<br>\n",
        "Optuna uses the codes from SciPy and fdlibm projects (see LICENSE_THIRD_PARTY).<br>"
      ],
      "metadata": {
        "id": "sRwQGbFxEe-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "captum:<br>\n",
        "License:<br>\n",
        "Captum is BSD licensed, as found in the LICENSE file.<br>"
      ],
      "metadata": {
        "id": "Zkczw-C_H0ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rectified Flow:<br>\n",
        "paper: https://arxiv.org/abs/2209.03003<br>\n",
        "github: https://github.com/gnobitab/RectifiedFlow<br>"
      ],
      "metadata": {
        "id": "1XFxXN90cXAc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vCr3ZujbH5b"
      },
      "source": [
        "project description:<br>\n",
        "add DiT before the control net input blocks.<br>\n",
        "adjust to Rectified Flow.<br>\n",
        "Github: https://github.com/Allen33669/control_net_dit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-O071a2CuQa"
      },
      "source": [
        "install condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN8NMfWYClXL"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAzMwyx1UGk7"
      },
      "outputs": [],
      "source": [
        "import condacolab\n",
        "condacolab.check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3u3qfmeCJKL"
      },
      "source": [
        "download the source code.\n",
        "ControlNet, DiT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1CajWTEB27J"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/lllyasviel/ControlNet.git\n",
        "%cd /content/ControlNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1itJrsjCN-z"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/DiT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my_7qfZLtiYb"
      },
      "source": [
        "update<br>\n",
        "/content/ControlNet/cldm/cldm.py<br>\n",
        "/content/ControlNet/cldm/ddim_hacked.py<br>\n",
        "/content/ControlNet/models/cldm_v15.yaml (This is so that you can just use an L4 to execute, you can also use original cldm_v15.yaml)<br>\n",
        "/content/ControlNet/ldm/models/diffusion/ddpm.py<br>\n",
        "/content/ControlNet/tool_add_control.py<br>\n",
        "/content/ControlNet/my_dataset_infer.py<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKKZDiPLCdRs"
      },
      "source": [
        "upload<br>\n",
        "/content/ControlNet/control_net_DiT_environment_2.yml<br>\n",
        "/content/ControlNet/environment_2.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM73zKdpH85q"
      },
      "source": [
        "editted the pinned file /usr/local/conda-meta/pinned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LTnyGKuRC1pi"
      },
      "outputs": [],
      "source": [
        "!conda env update -n base -f control_net_DiT_environment_2.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmpQMMuk22TH"
      },
      "outputs": [],
      "source": [
        "!conda --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mno-dsxvFWga"
      },
      "outputs": [],
      "source": [
        "!pip install -r environment_2.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3rzJH25glDxQ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade einops\n",
        "!pip install matplotlib\n",
        "!pip install optuna\n",
        "!pip install captum\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hohWpCcJai4o"
      },
      "outputs": [],
      "source": [
        "!conda list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KpDKQrudJNY"
      },
      "source": [
        "add package path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4wArZ2wZ94i"
      },
      "outputs": [],
      "source": [
        "sys.path.append('/usr/local/lib/python3.8/site-packages')\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObMg_6tMbRWN"
      },
      "outputs": [],
      "source": [
        "%cd /content/ControlNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1R5OFX3Ysx0"
      },
      "source": [
        "import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldj3LQPeYx6z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "import zipfile\n",
        "import json\n",
        "from google.colab.patches import cv2_imshow\n",
        "import config\n",
        "import cv2\n",
        "import einops\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from pytorch_lightning.callbacks import DeviceStatsMonitor\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "import optuna\n",
        "from optuna.importance import get_param_importances\n",
        "from optuna.visualization import *\n",
        "\"\"\"\n",
        "from optuna.visualization import plot_contour\n",
        "from optuna.visualization import plot_edf\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_parallel_coordinate\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_rank\n",
        "from optuna.visualization import plot_slice\n",
        "from optuna.visualization import plot_timeline\n",
        "\"\"\"\n",
        "from captum._utils.models.linear_model import SkLearnLinearModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from captum.attr import *\n",
        "\n",
        "import gradio as gr\n",
        "from share import *\n",
        "from annotator.util import resize_image, HWC3\n",
        "from annotator.canny import CannyDetector\n",
        "from cldm.model import create_model, load_state_dict\n",
        "from cldm.ddim_hacked import DDIMSampler\n",
        "from tutorial_dataset import MyDataset\n",
        "from cldm.logger import ImageLogger\n",
        "from cldm.model import create_model, load_state_dict\n",
        "\n",
        "\n",
        "from my_dataset_infer import MyDatasetInfer\n",
        "from cldm.cldm import XAIForwardImage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vHwnTq-9Ys_i"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwIzX-J_ccUz"
      },
      "source": [
        "functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVdxZaVAcbV7"
      },
      "outputs": [],
      "source": [
        "# This function is from https://github.com/lllyasviel/controlnet\n",
        "def process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold):\n",
        "    with torch.no_grad():\n",
        "        img = resize_image(HWC3(input_image), image_resolution)\n",
        "        H, W, C = img.shape\n",
        "\n",
        "        detected_map = apply_canny(img, low_threshold, high_threshold)\n",
        "        detected_map = HWC3(detected_map)\n",
        "\n",
        "        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n",
        "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
        "        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
        "\n",
        "        if seed == -1:\n",
        "            seed = random.randint(0, 65535)\n",
        "        seed_everything(seed)\n",
        "\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=False)\n",
        "\n",
        "        print(control.device)\n",
        "\n",
        "        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n",
        "        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n",
        "        shape = (4, H // 8, W // 8)\n",
        "\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=True)\n",
        "\n",
        "        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
        "        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n",
        "                                                     shape, cond, verbose=False, eta=eta,\n",
        "                                                     unconditional_guidance_scale=scale,\n",
        "                                                     unconditional_conditioning=un_cond)\n",
        "\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=False)\n",
        "\n",
        "        x_samples = model.decode_first_stage(samples)\n",
        "        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
        "\n",
        "        results = [x_samples[i] for i in range(num_samples)]\n",
        "    return [255 - detected_map] + results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00NfrNDRUXCo"
      },
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_-heLJbUbSr"
      },
      "source": [
        "dataset > download and unzip dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2zEOB0OUcKq"
      },
      "outputs": [],
      "source": [
        "directory_name = \"training\"\n",
        "os.makedirs(directory_name, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ba1YqKCKUg1G"
      },
      "outputs": [],
      "source": [
        "#download dataset\n",
        "repo_id = \"lllyasviel/ControlNet\"\n",
        "filename = \"training/fill50k.zip\"\n",
        "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "print(\"Downloaded file path:\", file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7kM0ERbUkdS"
      },
      "outputs": [],
      "source": [
        "#unzip the dataset\n",
        "extract_dir = \"/content/ControlNet/training\"\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "   zip_ref.extractall(extract_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgHjq2VJUmY7"
      },
      "source": [
        "dataset > explore dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bd6dkqwcUnFM"
      },
      "outputs": [],
      "source": [
        "print_img_data = []\n",
        "with open('./training/fill50k/prompt.json', 'rt') as f:\n",
        "  for line in f:\n",
        "    print_img_data.append(json.loads(line))\n",
        "\n",
        "item = print_img_data[321]\n",
        "\n",
        "source_filename = item['source']\n",
        "target_filename = item['target']\n",
        "prompt = item['prompt']\n",
        "\n",
        "source = cv2.imread('./training/fill50k/' + source_filename)\n",
        "target = cv2.imread('./training/fill50k/' + target_filename)\n",
        "\n",
        "print(prompt)\n",
        "cv2_imshow(source)\n",
        "cv2_imshow(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1yRMBrDXDz9"
      },
      "source": [
        "dataset > create and set dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avk_o7JhXF0G"
      },
      "outputs": [],
      "source": [
        "dataset = MyDataset()\n",
        "print(len(dataset))\n",
        "dataset.data = dataset.data[:20]\n",
        "print(len(dataset))\n",
        "\n",
        "item = dataset[3]\n",
        "jpg = item['jpg']\n",
        "txt = item['txt']\n",
        "hint = item['hint']\n",
        "print(txt)\n",
        "print(jpg.shape)\n",
        "print(hint.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB-MDjyoXJ6q"
      },
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTDaQjCuXLOZ"
      },
      "source": [
        "train > download ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qYzTLWqXQgK"
      },
      "outputs": [],
      "source": [
        "repo_id = \"botp/stable-diffusion-v1-5\"\n",
        "filename = \"v1-5-pruned.ckpt\"\n",
        "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "print(\"Downloaded file path:\", file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyG8u1K7XWHc"
      },
      "source": [
        "train > load pretrained params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGf0wB3UXgt5"
      },
      "outputs": [],
      "source": [
        "!python tool_add_control.py {file_path} ./models/control_sd15_ini.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aff3v7ryAyHD"
      },
      "source": [
        "train > find hyperparameters > find hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65qCi66_BBcC"
      },
      "outputs": [],
      "source": [
        "def objective(trial: optuna.trial.Trial) -> float:\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1)\n",
        "    l1_lambda = trial.suggest_float(\"l1_lambda\", 1e-5, 1)\n",
        "    l2_lambda = trial.suggest_float(\"l2_lambda\", 1e-5, 1)\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 1, 4)\n",
        "\n",
        "    resume_path = './models/control_sd15_ini.ckpt'\n",
        "    logger_freq = 1\n",
        "    sd_locked = True\n",
        "    only_mid_control = False\n",
        "\n",
        "    model = create_model('./models/cldm_v15.yaml')\n",
        "    model.load_state_dict(load_state_dict(resume_path, location='cpu'), strict=False)\n",
        "    model.learning_rate = learning_rate\n",
        "    model.sd_locked = sd_locked\n",
        "    model.only_mid_control = only_mid_control\n",
        "    model.l1_lambda = l1_lambda\n",
        "    model.l2_lambda = l2_lambda\n",
        "\n",
        "    dataloader = DataLoader(dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
        "    logger = ImageLogger(batch_frequency=logger_freq)\n",
        "    trainer = pl.Trainer(gpus=1, precision=32, callbacks=[logger], max_epochs=1, log_every_n_steps=2)\n",
        "\n",
        "    hyperparameters = dict(learning_rate=learning_rate)\n",
        "    trainer.logger.log_hyperparams(hyperparameters)\n",
        "    trainer.fit(model, dataloader)\n",
        "\n",
        "    return trainer.callback_metrics[\"train/loss\"].item(), trainer.callback_metrics[\"train/l1_regular_loss\"].item(), trainer.callback_metrics[\"train/l2_regular_loss\"].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfdV9prqBJKT"
      },
      "outputs": [],
      "source": [
        "    study = optuna.create_study(\n",
        "        directions=['minimize', 'minimize', 'minimize'],\n",
        "        sampler=optuna.samplers.NSGAIISampler()\n",
        "    )\n",
        "    print(f\"Sampler is {study.sampler.__class__.__name__}\")\n",
        "    study.enqueue_trial(\n",
        "      {\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"l1_lambda\": 1e-3,\n",
        "        \"l2_lambda\": 1e-3,\n",
        "      }\n",
        "    )\n",
        "    study.optimize(objective, n_trials=2)\n",
        "\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for trial in study.trials:\n",
        "  print('_'* 60)\n",
        "  print(f\"Trial {trial.number}:\")\n",
        "  print(f\" Params: {trial.params}\")\n",
        "  print(f\" Values: {trial.values}\")\n",
        "  print(f\" State: {trial.state}\")\n",
        "  print(f\" Duration: {trial.duration}\")\n",
        "  print(f\" User Attributes: {trial.user_attrs}\")\n",
        "  print(f\" System Attributes: {trial.system_attrs}\")\n",
        "\n",
        "importances = get_param_importances(study, target=lambda t: t.values[0])\n",
        "print('train/loss importances:')\n",
        "print(importances)\n",
        "\n",
        "importances = get_param_importances(study, target=lambda t: t.values[1])\n",
        "print('train/l1_regular_loss:')\n",
        "print(importances)\n",
        "\n",
        "importances = get_param_importances(study, target=lambda t: t.values[2])\n",
        "print('train/l2_regular_loss:')\n",
        "print(importances)\n"
      ],
      "metadata": {
        "id": "Lafu7T8M4KUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train > find hyperparameters > visualize trial and hyperparameters"
      ],
      "metadata": {
        "id": "x2OHt4mj81LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plot_param_importances(study)\n",
        "fig.show()\n",
        "fig = plot_optimization_history(study, target=lambda t: t.values[0])\n",
        "fig.show()\n",
        "fig = plot_optimization_history(study, target=lambda t: t.values[1])\n",
        "fig.show()\n",
        "fig = plot_optimization_history(study, target=lambda t: t.values[2])\n",
        "fig.show()\n",
        "fig = plot_intermediate_values(study)\n",
        "fig.show()\n",
        "fig = plot_parallel_coordinate(study, target=lambda t: t.values[0])\n",
        "fig.show()\n",
        "fig = plot_parallel_coordinate(study, target=lambda t: t.values[1])\n",
        "fig.show()\n",
        "fig = plot_parallel_coordinate(study, target=lambda t: t.values[2])\n",
        "fig.show()\n",
        "fig = plot_parallel_coordinate(study, params=[\"learning_rate\", \"l1_lambda\", \"l2_lambda\"], target=lambda t: t.values[0])\n",
        "fig.show()\n",
        "fig = plot_parallel_coordinate(study, params=[\"learning_rate\", \"l1_lambda\", \"l2_lambda\"], target=lambda t: t.values[1])\n",
        "fig.show()\n",
        "fig = plot_parallel_coordinate(study, params=[\"learning_rate\", \"l1_lambda\", \"l2_lambda\"], target=lambda t: t.values[2])\n",
        "fig.show()\n",
        "fig = plot_contour(study, target=lambda t: t.values[0])\n",
        "fig.show()\n",
        "fig = plot_contour(study, target=lambda t: t.values[1])\n",
        "fig.show()\n",
        "fig = plot_contour(study, target=lambda t: t.values[2])\n",
        "fig.show()\n",
        "fig = plot_slice(study, target=lambda t: t.values[0])\n",
        "fig.show()\n",
        "fig = plot_slice(study, target=lambda t: t.values[1])\n",
        "fig.show()\n",
        "fig = plot_slice(study, target=lambda t: t.values[2])\n",
        "fig.show()\n",
        "fig = plot_edf(study, target=lambda t: t.values[0])\n",
        "fig.show()\n",
        "fig = plot_edf(study, target=lambda t: t.values[1])\n",
        "fig.show()\n",
        "fig = plot_edf(study, target=lambda t: t.values[2])\n",
        "fig.show()\n",
        "fig = plot_rank(study, target=lambda t: t.values[0])\n",
        "fig.show()\n",
        "fig = plot_rank(study, target=lambda t: t.values[1])\n",
        "fig.show()\n",
        "fig = plot_rank(study, target=lambda t: t.values[2])\n",
        "fig.show()\n",
        "fig = plot_timeline(study)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "2H379d179LEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qth06bEwYsQT"
      },
      "source": [
        "train > Rectified Flow > first Rectified Flow > train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEMqC9eVY28F"
      },
      "outputs": [],
      "source": [
        "# This is adapted from https://github.com/lllyasviel/controlnet\n",
        "# Configs\n",
        "resume_path = './models/control_sd15_ini.ckpt'\n",
        "batch_size = 4\n",
        "logger_freq = 300\n",
        "learning_rate = 1e-5\n",
        "sd_locked = True\n",
        "only_mid_control = False\n",
        "\n",
        "\n",
        "# First use cpu to load models. Pytorch Lightning will automatically move it to GPUs.\n",
        "model = create_model('./models/cldm_v15.yaml')\n",
        "model.load_state_dict(load_state_dict(resume_path, location='cpu'), strict=False)\n",
        "model.learning_rate = learning_rate\n",
        "model.sd_locked = sd_locked\n",
        "model.only_mid_control = only_mid_control\n",
        "\n",
        "\n",
        "# Misc\n",
        "dataloader = DataLoader(dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
        "logger = ImageLogger(batch_frequency=logger_freq)\n",
        "trainer = pl.Trainer(gpus=1, precision=32, callbacks=[logger, DeviceStatsMonitor()], max_epochs=1, log_every_n_steps=2)\n",
        "\n",
        "\n",
        "# Train!\n",
        "trainer.fit(model, dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRmZYZUfZEI7"
      },
      "source": [
        "train > save the model ckpt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZB5YFoHZHb6"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model_state_dict': model.state_dict() }\n",
        "torch.save(checkpoint, './models/sd15_control_net_DiT_fill50k_rectified_flow_1.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train > Rectified Flow > first Rectified Flow > generate samples"
      ],
      "metadata": {
        "id": "jZbqW9wVyStP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = './models/sd15_control_net_DiT_fill50k_rectified_flow_1.ckpt'\n",
        "ckpt = torch.load(ckpt_path)\n",
        "model = create_model('./models/cldm_v15.yaml')\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "model = model.cuda()\n",
        "\n",
        "apply_canny = CannyDetector()\n",
        "ddim_sampler = DDIMSampler(model)"
      ],
      "metadata": {
        "id": "jiadFaSTEav1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset = MyDatasetInfer()\n",
        "texts = []\n",
        "images = []\n",
        "images_target = []\n",
        "images_guided = []\n",
        "\n",
        "for i in range(5):\n",
        "  item = my_dataset[i]\n",
        "  input_image_target = item['jpg']\n",
        "  prompt = item['txt']\n",
        "  input_image = item['hint']\n",
        "\n",
        "  a_prompt = 'best quality, extremely detailed'\n",
        "  n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n",
        "  num_samples = 3\n",
        "  image_resolution = 512\n",
        "  ddim_steps = 20\n",
        "  guess_mode = False\n",
        "  strength = 1\n",
        "  scale = 9\n",
        "  seed = 10000\n",
        "  eta = 0\n",
        "  low_threshold = 100\n",
        "  high_threshold = 200\n",
        "\n",
        "  result = process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold)\n",
        "  print('_' * 60)\n",
        "  print(f'result:{len(result)}, {result[0].shape}')\n",
        "  for i in range(len(result)):\n",
        "    print(result[i].shape)\n",
        "    plt.imshow(result[i])\n",
        "    plt.axis('off') # Hide the axis\n",
        "    plt.show()\n",
        "\n",
        "  for i in range(1,len(result)):\n",
        "    texts.append(prompt)\n",
        "    images.append(result[i])\n",
        "    images_target.append(input_image_target)\n",
        "    images_guided.append(input_image)\n",
        "\n",
        "print('_' * 60)\n",
        "print(f'texts:{len(texts)}')\n",
        "print(texts[0])\n",
        "print(texts[1])\n",
        "print(texts[3])\n",
        "print(texts[10])\n",
        "print(f'images_guided:{len(images_guided)}, {images_guided[0].shape}')\n",
        "plt.imshow(images_guided[0])\n",
        "plt.show()\n",
        "plt.imshow(images_guided[1])\n",
        "plt.show()\n",
        "plt.imshow(images_guided[3])\n",
        "plt.show()\n",
        "plt.imshow(images_guided[10])\n",
        "plt.show()\n",
        "print(f'images:{len(images)}, {images[0].shape}')\n",
        "plt.imshow(images[0])\n",
        "plt.show()\n",
        "plt.imshow(images[1])\n",
        "plt.show()\n",
        "plt.imshow(images[3])\n",
        "plt.show()\n",
        "plt.imshow(images[10])\n",
        "plt.show()\n",
        "print(f'images_target:{len(images_target)}, {images_target[0].shape}')\n",
        "plt.imshow(images_target[0])\n",
        "plt.show()\n",
        "plt.imshow(images_target[1])\n",
        "plt.show()\n",
        "plt.imshow(images_target[3])\n",
        "plt.show()\n",
        "plt.imshow(images_target[10])\n",
        "plt.show()\n",
        "\n",
        "infer_prompt_guided_predict_target = [texts, images_guided, images, images_target]\n",
        "\"\"\"\n",
        "infer_prompt_guided_predict_target.pth: [texts, images_guided, images, images_target]\n",
        "texts: list of strings\n",
        "images_guided: list of guided images, each image is a numpy array of shape (512, 512, 3)\n",
        "images: list of predicted images, each image is a numpy array of shape (512, 512, 3)\n",
        "images_target: list of groud truth images, each image is a numpy array of shape (512, 512, 3)\n",
        "\"\"\"\n",
        "torch.save(infer_prompt_guided_predict_target, 'infer_prompt_guided_predict_target_rectified_flow_1.pth')"
      ],
      "metadata": {
        "id": "X6KKzNqAOVgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train > Rectified Flow > second Rectified Flow > reflow > train the model"
      ],
      "metadata": {
        "id": "MJLS9KV6yX8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDatasetRectifiedFlow(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        self.data = []\n",
        "        data = torch.load(data_path)\n",
        "        texts = data[0]\n",
        "        images_guided = data[1]\n",
        "        images_predict = data[2]\n",
        "        images_target = data[3]\n",
        "          for i in range(len(texts)):\n",
        "              item = {}\n",
        "              item['txt'] = texts[i]\n",
        "              item['hint'] = images_guided[i]\n",
        "              item['jpg'] = images_predict[i]\n",
        "              item['target'] = images_target[i]\n",
        "              self.data.append(item)\n",
        "\n",
        "        images_guided_tensor = torch.tensor(images_guided)\n",
        "        images_predict_tensor = torch.tensor(images_predict)\n",
        "        images_target_tensor = torch.tensor(images_target)\n",
        "\n",
        "        print('_' * 60)\n",
        "        print(f'texts:{len(texts)}')\n",
        "        print(texts[0])\n",
        "        print(texts[1])\n",
        "        print(texts[3])\n",
        "        print(texts[10])\n",
        "        print(f'images_guided:{len(images_guided)}, {images_guided[0].shape}')\n",
        "        print(f'images_guided:{images_guided[0]}')\n",
        "        plt.imshow(images_guided[0])\n",
        "        plt.show()\n",
        "        plt.imshow(images_guided[1])\n",
        "        plt.show()\n",
        "        plt.imshow(images_guided[3])\n",
        "        plt.show()\n",
        "        plt.imshow(images_guided[10])\n",
        "        plt.show()\n",
        "        print(f'images_predict:{len(images_predict)}, {images_predict[0].shape}')\n",
        "        print(f'images_guided:{images_predict[0]}')\n",
        "        plt.imshow(images_predict[0])\n",
        "        plt.show()\n",
        "        plt.imshow(images_predict[1])\n",
        "        plt.show()\n",
        "        plt.imshow(images_predict[3])\n",
        "        plt.show()\n",
        "        plt.imshow(images_predict[10])\n",
        "        plt.show()\n",
        "        print(f'images_target:{len(images_target)}, {images_target[0].shape}')\n",
        "        print(f'images_guided:{images_target[0]}')\n",
        "        plt.imshow(images_target[0])\n",
        "        plt.show()\n",
        "        plt.imshow(images_target[1])\n",
        "        plt.show()\n",
        "        plt.imshow(images_target[3])\n",
        "        plt.show()\n",
        "        plt.imshow(images_target[10])\n",
        "        plt.show()\n",
        "\n",
        "        print('_' * 60)\n",
        "        print(f'images_guided_tensor:{images_guided_tensor.shape}')\n",
        "        print(f'images_predict_tensor:{images_predict_tensor.shape}')\n",
        "        print(f'images_target_tensor:{images_target_tensor.shape}')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        source = item['hint']\n",
        "        target = item['jpg']\n",
        "        prompt = item['txt']\n",
        "\n",
        "        # Normalize source images to [0, 1].\n",
        "        source = source.astype(np.float32) / 255.0\n",
        "\n",
        "        # Normalize target images to [-1, 1].\n",
        "        target = (target.astype(np.float32) / 127.5) - 1.0\n",
        "\n",
        "        return dict(jpg=target, txt=prompt, hint=source)"
      ],
      "metadata": {
        "id": "bbIaczHAPTNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDatasetRectifiedFlow('infer_prompt_guided_predict_target_rectified_flow_1.pth')\n",
        "print(len(dataset))\n",
        "dataset.data = dataset.data[:20]\n",
        "print(len(dataset))\n",
        "\n",
        "item = dataset[3]\n",
        "jpg = item['jpg']\n",
        "txt = item['txt']\n",
        "hint = item['hint']\n",
        "print(txt)\n",
        "print(jpg.shape)\n",
        "print(hint.shape)"
      ],
      "metadata": {
        "id": "zgjoGPPcU-UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is adapted from https://github.com/lllyasviel/controlnet\n",
        "# Configs\n",
        "resume_path = './models/sd15_control_net_DiT_fill50k_rectified_flow_1.ckpt'\n",
        "batch_size = 4\n",
        "logger_freq = 300\n",
        "learning_rate = 1e-5\n",
        "sd_locked = True\n",
        "only_mid_control = False\n",
        "\n",
        "\n",
        "# First use cpu to load models. Pytorch Lightning will automatically move it to GPUs.\n",
        "model = create_model('./models/cldm_v15.yaml')\n",
        "model.load_state_dict(load_state_dict(resume_path, location='cpu'), strict=False)\n",
        "model.learning_rate = learning_rate\n",
        "model.sd_locked = sd_locked\n",
        "model.only_mid_control = only_mid_control\n",
        "\n",
        "\n",
        "# Misc\n",
        "dataloader = DataLoader(dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
        "logger = ImageLogger(batch_frequency=logger_freq)\n",
        "trainer = pl.Trainer(gpus=1, precision=32, callbacks=[logger, DeviceStatsMonitor()], max_epochs=1, log_every_n_steps=2)\n",
        "\n",
        "\n",
        "# Train!\n",
        "trainer.fit(model, dataloader)"
      ],
      "metadata": {
        "id": "--kglum6VZXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model_state_dict': model.state_dict() }\n",
        "torch.save(checkpoint, './models/sd15_control_net_DiT_fill50k_rectified_flow_2.ckpt')"
      ],
      "metadata": {
        "id": "j2WC3_zNVz74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train > Rectified Flow > second Rectified Flow > generate samples"
      ],
      "metadata": {
        "id": "aaREicJzyrqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = './models/sd15_control_net_DiT_fill50k_rectified_flow_2.ckpt'\n",
        "ckpt = torch.load(ckpt_path)\n",
        "model = create_model('./models/cldm_v15.yaml')\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "model = model.cuda()\n",
        "\n",
        "apply_canny = CannyDetector()\n",
        "ddim_sampler = DDIMSampler(model)"
      ],
      "metadata": {
        "id": "616lpgF0WhE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset = MyDatasetInfer()\n",
        "texts = []\n",
        "images = []\n",
        "images_target = []\n",
        "images_guided = []\n",
        "\n",
        "for i in range(5):\n",
        "  item = my_dataset[i]\n",
        "  input_image_target = item['jpg']\n",
        "  prompt = item['txt']\n",
        "  input_image = item['hint']\n",
        "\n",
        "  a_prompt = 'best quality, extremely detailed'\n",
        "  n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n",
        "  num_samples = 3\n",
        "  image_resolution = 512\n",
        "  ddim_steps = 20\n",
        "  guess_mode = False\n",
        "  strength = 1\n",
        "  scale = 9\n",
        "  seed = 10000\n",
        "  eta = 0\n",
        "  low_threshold = 100\n",
        "  high_threshold = 200\n",
        "\n",
        "  result = process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold)\n",
        "  print('_' * 60)\n",
        "  print(f'result:{len(result)}, {result[0].shape}')\n",
        "  for i in range(len(result)):\n",
        "    print(result[i].shape)\n",
        "    plt.imshow(result[i])\n",
        "    plt.axis('off') # Hide the axis\n",
        "    plt.show()\n",
        "\n",
        "  for i in range(1,len(result)):\n",
        "    texts.append(prompt)\n",
        "    images.append(result[i])\n",
        "    images_target.append(input_image_target)\n",
        "    images_guided.append(input_image)\n",
        "\n",
        "print('_' * 60)\n",
        "print(f'texts:{len(texts)}')\n",
        "print(texts[0])\n",
        "print(texts[1])\n",
        "print(texts[3])\n",
        "print(texts[10])\n",
        "print(f'images_guided:{len(images_guided)}, {images_guided[0].shape}')\n",
        "plt.imshow(images_guided[0])\n",
        "plt.show()\n",
        "plt.imshow(images_guided[1])\n",
        "plt.show()\n",
        "plt.imshow(images_guided[3])\n",
        "plt.show()\n",
        "plt.imshow(images_guided[10])\n",
        "plt.show()\n",
        "print(f'images:{len(images)}, {images[0].shape}')\n",
        "plt.imshow(images[0])\n",
        "plt.show()\n",
        "plt.imshow(images[1])\n",
        "plt.show()\n",
        "plt.imshow(images[3])\n",
        "plt.show()\n",
        "plt.imshow(images[10])\n",
        "plt.show()\n",
        "print(f'images_target:{len(images_target)}, {images_target[0].shape}')\n",
        "plt.imshow(images_target[0])\n",
        "plt.show()\n",
        "plt.imshow(images_target[1])\n",
        "plt.show()\n",
        "plt.imshow(images_target[3])\n",
        "plt.show()\n",
        "plt.imshow(images_target[10])\n",
        "plt.show()\n",
        "\n",
        "infer_prompt_guided_predict_target = [texts, images_guided, images, images_target]\n",
        "\"\"\"\n",
        "infer_prompt_guided_predict_target.pth: [texts, images_guided, images, images_target]\n",
        "texts: list of strings\n",
        "images_guided: list of guided images, each image is a numpy array of shape (512, 512, 3)\n",
        "images: list of predicted images, each image is a numpy array of shape (512, 512, 3)\n",
        "images_target: list of groud truth images, each image is a numpy array of shape (512, 512, 3)\n",
        "\"\"\"\n",
        "torch.save(infer_prompt_guided_predict_target, 'infer_prompt_guided_predict_target_rectified_flow_2.pth')"
      ],
      "metadata": {
        "id": "TTJCY45iWhE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train > Rectified Flow > distill<br>\n",
        "use the original model (can adjust euler step to 1 for distillation, if you want)"
      ],
      "metadata": {
        "id": "s6g5qiR1ygCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDatasetRectifiedFlow('infer_prompt_guided_predict_target_rectified_flow_2.pth')\n",
        "print(len(dataset))\n",
        "dataset.data = dataset.data[:20]\n",
        "print(len(dataset))\n",
        "\n",
        "item = dataset[3]\n",
        "jpg = item['jpg']\n",
        "txt = item['txt']\n",
        "hint = item['hint']\n",
        "print(txt)\n",
        "print(jpg.shape)\n",
        "print(hint.shape)"
      ],
      "metadata": {
        "id": "Sm_xQXB_XQJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is adapted from https://github.com/lllyasviel/controlnet\n",
        "# Configs\n",
        "resume_path = './models/sd15_control_net_DiT_fill50k_rectified_flow_2.ckpt'\n",
        "batch_size = 4\n",
        "logger_freq = 300\n",
        "learning_rate = 1e-5\n",
        "sd_locked = True\n",
        "only_mid_control = False\n",
        "\n",
        "\n",
        "# First use cpu to load models. Pytorch Lightning will automatically move it to GPUs.\n",
        "model = create_model('./models/cldm_v15.yaml')\n",
        "model.load_state_dict(load_state_dict(resume_path, location='cpu'), strict=False)\n",
        "model.learning_rate = learning_rate\n",
        "model.sd_locked = sd_locked\n",
        "model.only_mid_control = only_mid_control\n",
        "\n",
        "\n",
        "# Misc\n",
        "dataloader = DataLoader(dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
        "logger = ImageLogger(batch_frequency=logger_freq)\n",
        "trainer = pl.Trainer(gpus=1, precision=32, callbacks=[logger, DeviceStatsMonitor()], max_epochs=1, log_every_n_steps=2)\n",
        "\n",
        "\n",
        "# Train!\n",
        "trainer.fit(model, dataloader)"
      ],
      "metadata": {
        "id": "qeiYbmoEXQJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model_state_dict': model.state_dict() }\n",
        "torch.save(checkpoint, './models/sd15_control_net_DiT_fill50k.ckpt')"
      ],
      "metadata": {
        "id": "wGtuhzEEXQJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI > prepare sample and model"
      ],
      "metadata": {
        "id": "9MBpqeVOgWXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset()\n",
        "print(len(dataset))\n",
        "dataset.data = dataset.data[:20]\n",
        "print(len(dataset))\n",
        "\n",
        "item = dataset[3]\n",
        "jpg = item['jpg']\n",
        "txt = item['txt']\n",
        "hint = item['hint']\n",
        "print(txt)\n",
        "print(jpg.shape)\n",
        "print(hint.shape)\n",
        "\n",
        "batch_size = 1\n",
        "dataloader = DataLoader(dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
        "data_iter = iter(dataloader)\n",
        "sample_batch = next(data_iter)"
      ],
      "metadata": {
        "id": "yLyUShst8c0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = './models/sd15_control_net_DiT_fill50k.ckpt'\n",
        "ckpt = torch.load(ckpt_path)\n",
        "model = create_model('./models/cldm_v15.yaml')\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "model = model.cuda()\n",
        "model.lime_sample = sample_batch\n",
        "\n",
        "logger_freq = 300\n",
        "logger = ImageLogger(batch_frequency=logger_freq)\n",
        "trainer = pl.Trainer(gpus=1, precision=32, callbacks=[logger, DeviceStatsMonitor()], max_epochs=1, log_every_n_steps=2)"
      ],
      "metadata": {
        "id": "thAAFD9GgWX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI > Kernel SHAP > input text"
      ],
      "metadata": {
        "id": "RVXjyf4bgWX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = model.lime_sample['txt']\n",
        "input_text_words = input_text[0].split()\n",
        "input_text_words_tensor = torch.ones(len(input_text_words)).unsqueeze(0)\n",
        "\n",
        "ks = KernelShap(model.lime_text_0_1)\n",
        "attr_coefs = ks.attribute(input_text_words_tensor, n_samples=10)\n",
        "print(attr_coefs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g9hd01CEgWX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI > Kernel SHAP > input image square"
      ],
      "metadata": {
        "id": "d_T3EqDogWX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_hint = model.lime_sample['hint']\n",
        "print(f'input_hint.shape:{input_hint.shape}')\n",
        "\n",
        "#feature mask\n",
        "mask_size = 16\n",
        "feature_mask = torch.zeros_like(input_hint[0], dtype=torch.long)\n",
        "\n",
        "# Fill the feature mask with square regions\n",
        "for i in range(0, feature_mask.shape[0], mask_size):\n",
        "  for j in range(0, feature_mask.shape[1], mask_size):\n",
        "     for k in range(0, 3):\n",
        "       feature_mask[i:i+mask_size, j:j+mask_size, k] = (i // mask_size) * (feature_mask.shape[1] // mask_size) + (j // mask_size)\n",
        "\n",
        "feature_mask = feature_mask.unsqueeze_(0)\n",
        "\n",
        "#attribute\n",
        "ks = KernelShap(model.kernel_shap_image)\n",
        "attr_coefs = ks.attribute(input_hint, n_samples=10, feature_mask=feature_mask)\n",
        "print(attr_coefs)"
      ],
      "metadata": {
        "id": "lFCZQ5u0gWX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr_coefs_graph = attr_coefs.squeeze(0)\n",
        "attr_coefs_graph = attr_coefs_graph.permute(2, 0, 1)\n",
        "\n",
        "split_tensors = torch.split(attr_coefs_graph, 1, dim=0)\n",
        "for i, chunk in enumerate(split_tensors):\n",
        "  chunk = chunk.squeeze(0)\n",
        "  chunk = chunk.detach().numpy()\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(chunk, cmap='hot', interpolation='nearest')\n",
        "  plt.colorbar()\n",
        "  plt.title(\"Attributions\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "nT4Lb8l6gWX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI > LIME > input text"
      ],
      "metadata": {
        "id": "Dq6omm2vgWX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_kernel(\n",
        "     original_input,\n",
        "     perturbed_input,\n",
        "     perturbed_interpretable_input,\n",
        "     **kwargs):\n",
        "         # kernel_width will be provided to attribute as a kwarg\n",
        "         kernel_width = kwargs[\"kernel_width\"]\n",
        "         result = cosine_similarity(original_input, perturbed_input).squeeze(0)[0]\n",
        "         result = (result + 1) / 2\n",
        "         return result\n",
        "\n",
        "def perturb_func(\n",
        "     original_input,\n",
        "     **kwargs):\n",
        "     input = torch.randint(0, 2, original_input.squeeze(0).shape)\n",
        "     input = input.unsqueeze(0)\n",
        "     return input\n",
        "\n",
        "def to_interp_transform(curr_sample, original_inp, **kwargs):\n",
        "     return curr_sample"
      ],
      "metadata": {
        "id": "xZ3L6JmTgWX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = model.lime_sample['txt']\n",
        "input_text_words = input_text[0].split()\n",
        "input_text_words_tensor = torch.ones(len(input_text_words)).unsqueeze(0)\n",
        "\n",
        "lime_attr = LimeBase(model.lime_text_0_1,\n",
        "                         SkLearnLinearModel(\"linear_model.Ridge\"),\n",
        "                         similarity_func=similarity_kernel,\n",
        "                         perturb_func=perturb_func,\n",
        "                         perturb_interpretable_space=False,\n",
        "                         from_interp_rep_transform=None,\n",
        "                         to_interp_rep_transform=to_interp_transform)\n",
        "\n",
        "attr_coefs = lime_attr.attribute(input_text_words_tensor, kernel_width=1.1, n_samples=10)\n",
        "print(f'attr_coefs.shape:{attr_coefs.shape}')\n",
        "print(f'attr_coefs:{attr_coefs}')"
      ],
      "metadata": {
        "id": "yFiZuoR1gWX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI > LIME > input image"
      ],
      "metadata": {
        "id": "khWx1LkPgWX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_kernel(\n",
        "     original_input,\n",
        "     perturbed_input,\n",
        "     perturbed_interpretable_input,\n",
        "     **kwargs):\n",
        "         # kernel_width will be provided to attribute as a kwarg\n",
        "         kernel_width = kwargs[\"kernel_width\"]\n",
        "         result = cosine_similarity(original_input, perturbed_input).squeeze(0)[0]\n",
        "         result = (result + 1) / 2\n",
        "         return result\n",
        "\n",
        "def perturb_func(\n",
        "     original_input,\n",
        "     **kwargs):\n",
        "     input = original_input.squeeze()\n",
        "     noise = torch.floor(torch.randn_like(input) * 64)\n",
        "     input = noise + 128\n",
        "     input = torch.clamp(input, min=0, max=255)\n",
        "     input = input.unsqueeze(0)\n",
        "     return input\n",
        "\n",
        "def to_interp_transform(curr_sample, original_inp, **kwargs):\n",
        "     return curr_sample"
      ],
      "metadata": {
        "id": "_8hRGIy0gWX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_hint = model.lime_sample['hint']\n",
        "input_hint = input_hint.squeeze(0)\n",
        "input_hint = torch.flatten(input_hint)\n",
        "input_hint = input_hint.unsqueeze(0)\n",
        "\n",
        "lime_attr = LimeBase(model.lime_image,\n",
        "                         SkLearnLinearModel(\"linear_model.Ridge\"),\n",
        "                         similarity_func=similarity_kernel,\n",
        "                         perturb_func=perturb_func,\n",
        "                         perturb_interpretable_space=False,\n",
        "                         from_interp_rep_transform=None,\n",
        "                         to_interp_rep_transform=to_interp_transform)\n",
        "\n",
        "attr_coefs = lime_attr.attribute(input_hint, kernel_width=1.1, n_samples=10)"
      ],
      "metadata": {
        "id": "XzSZWArJgWX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr_coefs = attr_coefs.view(model.lime_sample['hint'].shape).squeeze().detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(attr_coefs, cmap='hot', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.title(\"Attributions\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VIZKY5W2gWX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI > Feature Ablation > input image"
      ],
      "metadata": {
        "id": "p_GO8rNr5hIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_hint = model.lime_sample['hint']\n",
        "print(f'input_hint.shape:{input_hint.shape}')\n",
        "\n",
        "#feature mask\n",
        "mask_size = 16\n",
        "feature_mask = torch.zeros_like(input_hint[0], dtype=torch.long)\n",
        "\n",
        "# Fill the feature mask with square regions\n",
        "for i in range(0, feature_mask.shape[0], mask_size):\n",
        "  for j in range(0, feature_mask.shape[1], mask_size):\n",
        "     for k in range(0, 3):\n",
        "       feature_mask[i:i+mask_size, j:j+mask_size, k] = (i // mask_size) * (feature_mask.shape[1] // mask_size) + (j // mask_size)\n",
        "\n",
        "feature_mask = feature_mask.unsqueeze_(0)\n",
        "\n",
        "feature_perm = FeatureAblation(model.kernel_shap_image)\n",
        "attr_coefs = feature_perm.attribute(input_hint, feature_mask=feature_mask)\n",
        "print(f'attr_coefs.shape: {attr_coefs.shape}')\n",
        "print(f'attr_coefs: {attr_coefs}')\n"
      ],
      "metadata": {
        "id": "aVA6gyBo5hyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr_coefs_graph = attr_coefs.squeeze(0)\n",
        "attr_coefs_graph = attr_coefs_graph.permute(2, 0, 1)\n",
        "\n",
        "split_tensors = torch.split(attr_coefs_graph, 1, dim=0)\n",
        "for i, chunk in enumerate(split_tensors):\n",
        "  chunk = chunk.squeeze(0)\n",
        "  chunk = chunk.detach().numpy()\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(chunk, cmap='hot', interpolation='nearest')\n",
        "  plt.colorbar()\n",
        "  plt.title(\"Attributions\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "f4tFh_Hz7gT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI > Layer Feature Ablation > input image"
      ],
      "metadata": {
        "id": "cI6ZLvF5-Kiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_hint = model.lime_sample['hint']\n",
        "print(f'input_hint.shape:{input_hint.shape}')\n",
        "\n",
        "#feature mask\n",
        "mask_size = 2\n",
        "feature_mask = torch.zeros((4, 64, 64), dtype=torch.long)\n",
        "\n",
        "# Fill the feature mask with square regions\n",
        "for k in range(0, feature_mask.shape[0], 1):\n",
        "  for i in range(0, feature_mask.shape[1], mask_size):\n",
        "    for j in range(0, feature_mask.shape[2], mask_size):\n",
        "      feature_mask[k, i:i+mask_size, j:j+mask_size] = (i // mask_size) * (feature_mask.shape[2] // mask_size) + (j // mask_size)\n",
        "\n",
        "feature_mask = feature_mask.to('cuda')\n",
        "\n",
        "ablator = LayerFeatureAblation(model.kernel_shap_image, model.model.diffusion_model.out[2])\n",
        "attr_coefs = ablator.attribute(input_hint, layer_mask=feature_mask)\n",
        "print(f'attr_coefs.shape: {attr_coefs.shape}')\n",
        "print(f'attr_coefs: {attr_coefs}')\n"
      ],
      "metadata": {
        "id": "eerhNjwZ-PT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr_coefs_graph = attr_coefs.squeeze(0)\n",
        "\n",
        "split_tensors = torch.split(attr_coefs_graph, 1, dim=0)\n",
        "for i, chunk in enumerate(split_tensors):\n",
        "  chunk = chunk.squeeze(0)\n",
        "  chunk = chunk.detach().cpu().numpy()\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(chunk, cmap='hot', interpolation='nearest')\n",
        "  plt.colorbar()\n",
        "  plt.title(\"Attributions\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "iEK1axff-8V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI > Neuron Feature Ablation > input image"
      ],
      "metadata": {
        "id": "jWIC4mJGFnxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_hint = model.lime_sample['hint']\n",
        "print(f'input_hint.shape:{input_hint.shape}')\n",
        "\n",
        "#feature mask\n",
        "mask_size = 16\n",
        "feature_mask = torch.zeros_like(input_hint[0], dtype=torch.long)\n",
        "\n",
        "# Fill the feature mask with square regions\n",
        "for i in range(0, feature_mask.shape[0], mask_size):\n",
        "  for j in range(0, feature_mask.shape[1], mask_size):\n",
        "     for k in range(0, 3):\n",
        "       feature_mask[i:i+mask_size, j:j+mask_size, k] = (i // mask_size) * (feature_mask.shape[1] // mask_size) + (j // mask_size)\n",
        "\n",
        "feature_mask = feature_mask.unsqueeze_(0)\n",
        "\n",
        "ablator = NeuronFeatureAblation(model.kernel_shap_image, model.model.diffusion_model.out[2])\n",
        "attr_coefs = ablator.attribute(input_hint, neuron_selector=(3,1,2), feature_mask=feature_mask)\n",
        "print(f'attr_coefs.shape: {attr_coefs.shape}')\n",
        "print(f'attr_coefs: {attr_coefs}')"
      ],
      "metadata": {
        "id": "jCkeTpe9Fsh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr_coefs_graph = attr_coefs.squeeze(0)\n",
        "attr_coefs_graph = attr_coefs_graph.permute(2, 0, 1)\n",
        "\n",
        "split_tensors = torch.split(attr_coefs_graph, 1, dim=0)\n",
        "for i, chunk in enumerate(split_tensors):\n",
        "  chunk = chunk.squeeze(0)\n",
        "  chunk = chunk.detach().numpy()\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(chunk, cmap='hot', interpolation='nearest')\n",
        "  plt.colorbar()\n",
        "  plt.title(\"Attributions\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "tCL8pgEs_r6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI > Feature Permutation > input image"
      ],
      "metadata": {
        "id": "CwaTmU7QuyoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_permutation_batch_size = 2\n",
        "feature_permutation_dataloader = DataLoader(dataset, num_workers=0, batch_size=feature_permutation_batch_size, shuffle=True)\n",
        "feature_permutation_data_iter = iter(feature_permutation_dataloader)\n",
        "feature_permutation_sample_batch = next(feature_permutation_data_iter)\n",
        "\n",
        "model.feature_permutation_sample = feature_permutation_sample_batch"
      ],
      "metadata": {
        "id": "5kU1ZQfNwNJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_hint = model.feature_permutation_sample['hint']\n",
        "print(f'input_hint.shape:{input_hint.shape}')\n",
        "\n",
        "#feature mask\n",
        "mask_size = 16\n",
        "feature_mask = torch.zeros_like(input_hint[0], dtype=torch.long)\n",
        "\n",
        "# Fill the feature mask with square regions\n",
        "for i in range(0, feature_mask.shape[0], mask_size):\n",
        "  for j in range(0, feature_mask.shape[1], mask_size):\n",
        "     for k in range(0, 3):\n",
        "       feature_mask[i:i+mask_size, j:j+mask_size, k] = (i // mask_size) * (feature_mask.shape[1] // mask_size) + (j // mask_size)\n",
        "\n",
        "feature_mask = feature_mask.unsqueeze_(0)\n",
        "\n",
        "feature_perm = FeaturePermutation(model.kernel_shap_image)\n",
        "attr_coefs = feature_perm.attribute(input_hint, feature_mask=feature_mask)\n",
        "print(f'attr_coefs.shape: {attr_coefs.shape}')\n",
        "print(f'attr_coefs: {attr_coefs}')"
      ],
      "metadata": {
        "id": "vffp6kR9uzjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr_coefs_graph = attr_coefs.squeeze(0)\n",
        "attr_coefs_graph = attr_coefs_graph.permute(2, 0, 1)\n",
        "\n",
        "split_tensors = torch.split(attr_coefs_graph, 1, dim=0)\n",
        "for i, chunk in enumerate(split_tensors):\n",
        "  chunk = chunk.squeeze(0)\n",
        "  chunk = chunk.detach().numpy()\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(chunk, cmap='hot', interpolation='nearest')\n",
        "  plt.colorbar()\n",
        "  plt.title(\"Attributions\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "RJjKg_A8yOcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZm33wv4ZJaq"
      },
      "source": [
        "infer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_108VjErqJsC"
      },
      "outputs": [],
      "source": [
        "ckpt_path = './models/sd15_control_net_DiT_fill50k.ckpt'\n",
        "ckpt = torch.load(ckpt_path)\n",
        "model = create_model('./models/cldm_v15.yaml')\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "model = model.cuda()\n",
        "\n",
        "apply_canny = CannyDetector()\n",
        "ddim_sampler = DDIMSampler(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vZMiJ42QZLoo"
      },
      "outputs": [],
      "source": [
        "my_dataset = MyDatasetInfer()\n",
        "item = my_dataset[123]\n",
        "input_image_target = item['jpg']\n",
        "prompt = item['txt']\n",
        "input_image = item['hint']\n",
        "print(prompt)\n",
        "print(input_image_target.shape)\n",
        "print(input_image.shape)\n",
        "\n",
        "a_prompt = 'best quality, extremely detailed'\n",
        "n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n",
        "num_samples = 3\n",
        "image_resolution = 512\n",
        "ddim_steps = 20\n",
        "guess_mode = False\n",
        "strength = 1\n",
        "scale = 9\n",
        "seed = 10000\n",
        "eta = 0\n",
        "low_threshold = 100\n",
        "high_threshold = 200\n",
        "\n",
        "result = process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold)\n",
        "print('result:')\n",
        "print(result)\n",
        "for i in range(len(result)):\n",
        "  print(result[i].shape)\n",
        "  plt.imshow(result[i])\n",
        "  plt.axis('off') # Hide the axis\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNUoJlizTX5p"
      },
      "source": [
        "prepare evaluation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fqC-3_MoTcGY"
      },
      "outputs": [],
      "source": [
        "my_dataset = MyDatasetInfer()\n",
        "texts = []\n",
        "images = []\n",
        "images_target = []\n",
        "images_guided = []\n",
        "\n",
        "for i in range(5):\n",
        "  item = my_dataset[i]\n",
        "  input_image_target = item['jpg']\n",
        "  prompt = item['txt']\n",
        "  input_image = item['hint']\n",
        "\n",
        "  a_prompt = 'best quality, extremely detailed'\n",
        "  n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n",
        "  num_samples = 3\n",
        "  image_resolution = 512\n",
        "  ddim_steps = 20\n",
        "  guess_mode = False\n",
        "  strength = 1\n",
        "  scale = 9\n",
        "  seed = 10000\n",
        "  eta = 0\n",
        "  low_threshold = 100\n",
        "  high_threshold = 200\n",
        "\n",
        "  result = process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold)\n",
        "  print('_' * 60)\n",
        "  print(f'result:{len(result)}, {result[0].shape}')\n",
        "  for i in range(len(result)):\n",
        "    print(result[i].shape)\n",
        "    plt.imshow(result[i])\n",
        "    plt.axis('off') # Hide the axis\n",
        "    plt.show()\n",
        "\n",
        "  for i in range(1,len(result)):\n",
        "    texts.append(prompt)\n",
        "    images.append(result[i])\n",
        "    images_target.append(input_image_target)\n",
        "    images_guided.append(input_image)\n",
        "\n",
        "print('_' * 60)\n",
        "print(f'texts:{len(texts)}')\n",
        "print(texts[0])\n",
        "print(texts[1])\n",
        "print(texts[3])\n",
        "print(texts[10])\n",
        "print(f'images_guided:{len(images_guided)}, {images_guided[0].shape}')\n",
        "plt.imshow(images_guided[0])\n",
        "plt.show()\n",
        "plt.imshow(images_guided[1])\n",
        "plt.show()\n",
        "plt.imshow(images_guided[3])\n",
        "plt.show()\n",
        "plt.imshow(images_guided[10])\n",
        "plt.show()\n",
        "print(f'images:{len(images)}, {images[0].shape}')\n",
        "plt.imshow(images[0])\n",
        "plt.show()\n",
        "plt.imshow(images[1])\n",
        "plt.show()\n",
        "plt.imshow(images[3])\n",
        "plt.show()\n",
        "plt.imshow(images[10])\n",
        "plt.show()\n",
        "print(f'images_target:{len(images_target)}, {images_target[0].shape}')\n",
        "plt.imshow(images_target[0])\n",
        "plt.show()\n",
        "plt.imshow(images_target[1])\n",
        "plt.show()\n",
        "plt.imshow(images_target[3])\n",
        "plt.show()\n",
        "plt.imshow(images_target[10])\n",
        "plt.show()\n",
        "\n",
        "infer_prompt_guided_predict_target = [texts, images_guided, images, images_target]\n",
        "\"\"\"\n",
        "infer_prompt_guided_predict_target.pth: [texts, images_guided, images, images_target]\n",
        "texts: list of strings\n",
        "images_guided: list of guided images, each image is a numpy array of shape (512, 512, 3)\n",
        "images: list of predicted images, each image is a numpy array of shape (512, 512, 3)\n",
        "images_target: list of groud truth images, each image is a numpy array of shape (512, 512, 3)\n",
        "\"\"\"\n",
        "torch.save(infer_prompt_guided_predict_target, 'infer_prompt_guided_predict_target.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "logging > tensorboard"
      ],
      "metadata": {
        "id": "0BVnEPsh_POu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tb-nightly tensorboard tensorflow tensorflow-estimator\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "9i5cZpg3_PtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=/content/ControlNet/lightning_logs"
      ],
      "metadata": {
        "id": "MrBPEjmP_Y3m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}